{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1： 模型理论与应用\n",
    "在第一部分主要以证明和推导为主。以下几个问题都是比较经典的问题，会对模型的深入理解会有很大的帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 逻辑回归相关 (30分)\n",
    "假设我们有训练数据$D=\\{(\\mathbf{x}_1,y_1),...,(\\mathbf{x}_n,y_n)\\}$, 其中$(\\mathbf{x}_i,y_i)$为每一个样本，而且$\\mathbf{x}_i$是样本的特征并且$\\mathbf{x}_i\\in \\mathcal{R}^D$, $y_i$代表样本数据的标签（label）, 取值为$0$或者$1$. 在逻辑回归中，模型的参数为$(\\mathbf{w},b)$。对于向量，我们一般用粗体来表达。请回答以下问题。最好用Markdown自带的Latex来编写。（如果实在不行，可以手写然后拍照完放入word或者转成PDF，作为独立的文件来提交）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) 在逻辑回归模型下，请写出目标函数（objective function）, 也就是我们需要\"最小化\"的目标（也称之为损失函数或者loss function)，不需要考虑正则 （3分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L(\\mathbf{w},b)=-\\sum_{i=1}^{n}(y_i log(\\frac{1}{1+exp(w^Tx_i+b)})+(1-y_i)log(1-\\frac{1}{1+exp(w^Tx_i+b)}))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) 求出$L(\\mathbf{w},b)$的梯度（或者计算导数），需要必要的中间过程。（5分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned}\\frac{\\partial L(\\mathbf{w},b)}{\\partial \\mathbf{w}}\n",
    "&=-\\sum_{i=1}^{n}(y_i \\frac{\\partial }{\\partial \\mathbf{w}} \\log(\\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)})+(1-y_i) \\frac{\\partial }{\\partial \\mathbf{w}} \\log(1-\\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)}))\\\\\n",
    "&=-\\sum_{i=1}^{n}[y_i (1+\\exp(\\mathbf{w}^Tx_i+b)) \\frac{\\partial }{\\partial \\mathbf{w}} \\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)}-(1-y_i) \\frac{1+\\exp(\\mathbf{w}^Tx_i+b)}{\\exp(\\mathbf{w}^Tx_i+b)} \\frac{\\partial }{\\partial \\mathbf{w}} \\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)}]\\\\\n",
    "&=-\\sum_{i=1}^{n}(y_i (1+\\exp(\\mathbf{w}^Tx_i+b))-(1-y_i) \\frac{1+\\exp(\\mathbf{w}^Tx_i+b)}{\\exp(\\mathbf{w}^Tx_i+b)}) \\frac{\\partial }{\\partial \\mathbf{w}} \\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)}\\\\\n",
    "&=-\\sum_{i=1}^{n}(y_i (1+\\exp(\\mathbf{w}^Tx_i+b))-(1-y_i) \\frac{1+\\exp(\\mathbf{w}^Tx_i+b)}{\\exp(\\mathbf{w}^Tx_i+b)}) \\frac{1}{(1+\\exp(\\mathbf{w}^Tx_i+b))^2}\\frac{\\partial }{\\partial \\mathbf{w}}\\exp(\\mathbf{w}^Tx_i+b)\\\\\n",
    "&=-\\sum_{i=1}^{n}(y_i- \\frac{(1-y_i)}{\\exp(\\mathbf{w}^Tx_i+b)}) \\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)}\\frac{\\partial }{\\partial \\mathbf{w}}\\exp(\\mathbf{w}^Tx_i+b)\\\\\n",
    "&=-\\sum_{i=1}^{n}(y_i- \\frac{(1-y_i)}{\\exp(\\mathbf{w}^Tx_i+b)}) \\frac{\\exp(\\mathbf{w}^Tx_i+b)}{1+\\exp(\\mathbf{w}^Tx_i+b)}\\frac{\\partial }{\\partial \\mathbf{w}}(\\mathbf{w}^Tx_i+b)\\\\\n",
    "&=-\\sum_{i=1}^{n} \\frac{y_i \\exp(\\mathbf{w}^Tx_i+b)- (1-y_i)}{1+\\exp(\\mathbf{w}^Tx_i+b)}\\frac{\\partial }{\\partial \\mathbf{w}}(\\mathbf{w}^Tx_i+b)\\\\\n",
    "&=-\\sum_{i=1}^{n} \\frac{y_i (1+\\exp(\\mathbf{w}^Tx_i+b)) - 1}{1+\\exp(\\mathbf{w}^Tx_i+b)}\\frac{\\partial }{\\partial \\mathbf{w}}(\\mathbf{w}^Tx_i+b)\\\\\n",
    "&=-\\sum_{i=1}^{n} (y_i - \\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)})\\frac{\\partial }{\\partial \\mathbf{w}}(\\mathbf{w}^Tx_i+b)\\\\\n",
    "&=\\sum_{i=1}^{n} (\\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)} - y_i)\\frac{\\partial (\\mathbf{w}^Tx_i+b)}{\\partial \\mathbf{w}}\\\\\n",
    "&=\\sum_{i=1}^{n} (\\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)} - y_i)\\frac{\\partial \\mathbf{w}^Tx_i}{\\partial \\mathbf{w}}\\\\\n",
    "&=\\sum_{i=1}^{n} (\\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)} - y_i) x_i\\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned}\\frac{\\partial L(\\mathbf{w},b)}{\\partial b}\n",
    "&=\\sum_{i=1}^{n} (\\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)} - y_i)\\frac{\\partial (\\mathbf{w}^Tx_i+b)}{\\partial b}\\\\\n",
    "&=\\sum_{i=1}^{n} (\\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)} - y_i)\\frac{\\partial b}{\\partial b}\\\\\n",
    "&=\\sum_{i=1}^{n} (\\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)} - y_i)\\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(c) 请写出基于梯度下降法（batch）的对于$\\mathbf{w}$和$b$的更新 （5分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{w}^{t+1}=\\mathbf{w}^t -\\alpha \\sum_{i=1}^{n} (\\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)} - y_i) x_i$\n",
    "\n",
    "$b^{t+1}=b^t - \\alpha \\sum_{i=1}^{n} (\\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)} - y_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(d) 假设在(a)的基础上加了一个L2正则项，请写出基于梯度下降法（batch）的对于$\\mathbf{w}$和$b$的更新 （5分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\mathbf{w}^{t+1}=\\mathbf{w}^t -\\alpha (\\sum_{i=1}^{n} (\\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)} - y_i) x_i + \\lambda \\mathbf{w})$\n",
    "\n",
    "$b^{t+1}=b^t - \\alpha (\\sum_{i=1}^{n} (\\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)} - y_i)+\\lambda)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们来证明逻辑回归函数是凸函数。假设一个函数是凸函数，我们则可以得出局部最优解即为全局最优解，所以假设我们通过随机梯度下降法等手段找到最优解\n",
    "时我们就可以确认这个解就是全局最优解。证明凸函数的方法有很多种，在这里我们介绍一种方法，就是基于二次求导大于等于0。比如给定一个函数$f(x)=x^2-3x+3$，做两次\n",
    "求导之后即可以得出$f''(x)=2 > 0$，所以这个函数就是凸函数。类似的，这种理论也应用于多元变量中的函数上。在多元函数上，只要证明二阶导数是posititive semidefinite即可以。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(e) 在(b)的基础上接着对$\\mathbf{w}$求导（等于二阶导数，二阶导数的维度为$D\\times D$），这个二阶导数也称之为Hessian Matrix(https://en.wikipedia.org/wiki/Hessian_matrix) 对于矩阵、向量的求导请参考：https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf （8分）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\begin{aligned}\\frac{\\partial^2 \\mathcal{L}}{\\partial^2 \\mathbf{w}}\n",
    "&=\\frac{\\partial }{\\partial \\mathbf{w}} \\sum_{i=1}^{n} (\\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)} - y_i) x_i\\\\\n",
    "&=\\sum_{i=1}^{n} x_i \\frac{\\partial }{\\partial \\mathbf{w}} \\frac{1}{1+\\exp(\\mathbf{w}^Tx_i+b)} \\\\\n",
    "&=\\sum_{i=1}^{n}  \\frac{x_i}{(1+\\exp(\\mathbf{w}^Tx_i+b))^2} \\frac{\\partial }{\\partial \\mathbf{w}} \\exp(\\mathbf{w}^Tx_i+b) \\\\\n",
    "&=\\sum_{i=1}^{n}  \\frac{x_i \\exp(\\mathbf{w}^Tx_i+b)}{(1+\\exp(\\mathbf{w}^Tx_i+b))^2} \\frac{\\partial }{\\partial \\mathbf{w}}(\\mathbf{w}^Tx_i+b) \\\\\n",
    "&=\\sum_{i=1}^{n}  \\frac{x_i \\exp(\\mathbf{w}^Tx_i+b)}{(1+\\exp(\\mathbf{w}^Tx_i+b))^2} \\frac{\\partial (\\mathbf{w}^Tx_i+b)}{\\partial \\mathbf{w}} \\\\\n",
    "&=\\sum_{i=1}^{n}  \\frac{x_i \\exp(\\mathbf{w}^Tx_i+b)}{(1+\\exp(\\mathbf{w}^Tx_i+b))^2} x_i \\\\\n",
    "&=\\sum_{i=1}^{n}  \\frac{x_i^2 \\exp(\\mathbf{w}^Tx_i+b)}{(1+\\exp(\\mathbf{w}^Tx_i+b))^2}\\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(f) 请说明在(e)的得出来的Hessian Matrix是Positive Definite. 提示：为了证明一个$D\\times D$的矩阵$H$为Positive Semidefinite，需要证明对于任意一个非零向量$v\\in \\mathcal{R}^D$, 需要得出$v^{T}Hv >=0$ （4分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请推导或者说明：\n",
    "\n",
    "$\\begin{aligned}v^{T}Hv &= v^T\\frac{\\partial^2 \\mathcal{L}}{\\partial^2 \\mathbf{w}}v\\\\\n",
    "&=v^T\\sum_{i=1}^{n} \\frac{x_i^2 \\exp(\\mathbf{w}^Tx_i+b)}{(1+\\exp(\\mathbf{w}^Tx_i+b))^2}v\\\\\n",
    "&=\\sum_{i=1}^{n} \\frac{v^T x_i^2 \\exp(\\mathbf{w}^Tx_i+b) v}{(1+\\exp(\\mathbf{w}^Tx_i+b))^2}\\\\\n",
    "&=\\sum_{i=1}^{n} \\frac{v^T v x_i^2 \\exp(\\mathbf{w}^Tx_i+b) }{(1+\\exp(\\mathbf{w}^Tx_i+b))^2}\\\\\n",
    "&=\\sum_{i=1}^{n} \\frac{||v||^2 x_i^2 \\exp(\\mathbf{w}^Tx_i+b) }{(1+\\exp(\\mathbf{w}^Tx_i+b))^2}\\\\\n",
    "&>=0\\\\\n",
    "\\end{aligned}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2： 情感分析项目 (70分)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本项目的目标是基于用户提供的评论，通过算法自动去判断其评论是正面的还是负面的情感。比如给定一个用户的评论：\n",
    "- 评论1： “我特别喜欢这个电器，我已经用了3个月，一点问题都没有！”\n",
    "- 评论2： “我从这家淘宝店卖的东西不到一周就开始坏掉了，强烈建议不要买，真实浪费钱”\n",
    "\n",
    "对于这两个评论，第一个明显是正面的，第二个是负面的。 我们希望搭建一个AI算法能够自动帮我们识别出评论是正面还是负面。\n",
    "\n",
    "情感分析的应用场景非常丰富，也是NLP技术在不同场景中落地的典范。比如对于一个证券领域，作为股民，其实比较关注舆论的变化，这个时候如果能有一个AI算法自动给网络上的舆论做正负面判断，然后把所有相关的结论再整合，这样我们可以根据这些大众的舆论，辅助做买卖的决策。 另外，在电商领域评论无处不在，而且评论已经成为影响用户购买决策的非常重要的因素，所以如果AI系统能够自动分析其情感，则后续可以做很多有意思的应用。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "情感分析是文本处理领域经典的问题。整个系统一般会包括几个模块：\n",
    "- 数据的抓取： 通过爬虫的技术去网络抓取相关文本数据\n",
    "- 数据的清洗/预处理：在本文中一般需要去掉无用的信息，比如各种标签（HTML标签），标点符号，停用词等等\n",
    "- 把文本信息转换成向量： 这也成为特征工程，文本本身是不能作为模型的输入，只有数字（比如向量）才能成为模型的输入。所以进入模型之前，任何的信号都需要转换成模型可识别的数字信号（数字，向量，矩阵，张量...)\n",
    "- 选择合适的模型以及合适的评估方法。 对于情感分析来说，这是二分类问题（或者三分类：正面，负面，中性），所以需要采用分类算法比如逻辑回归，朴素贝叶斯，神经网络，SVM等等。另外，我们需要选择合适的评估方法，比如对于一个应用，我们是关注准确率呢，还是关注召回率呢？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本次项目中，我们已经给定了训练数据和测试数据，它们分别是 train.positive.txt, train.negative.txt， test_combined.txt. 请注意训练数据和测试数据的格式不一样，详情请见文件内容。 整个项目你需要完成以下步骤：\n",
    "\n",
    "数据的读取以及清洗： 从给定的.txt中读取内容，并做一些数据清洗，这里需要做几个工作： （1） 文本的读取，需要把字符串内容读进来。 （2）去掉无用的字符比如标点符号，多余的空格，换行符等 （3） 分词\n",
    "把文本转换成TF-IDF向量： 这部分直接可以利用sklearn提供的TfidfVectorizer类来做。\n",
    "利用逻辑回归模型来做分类，并通过交叉验证选择最合适的超参数\n",
    "利用支持向量机做分类，并通过交叉验证选择神经网络的合适的参数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Reading: 文本读取 （5分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import jieba\n",
    "import itertools\n",
    "import xml.etree.ElementTree\n",
    "import lxml.etree\n",
    "import matplotlib.pyplot\n",
    "import math\n",
    "import random\n",
    "import operator\n",
    "import string\n",
    "import re\n",
    "import numpy\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.linear_model\n",
    "import sklearn.model_selection\n",
    "import bayes_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_name):\n",
    "    doc = open(file_name,'r',encoding='utf-8-sig')\n",
    "    it = itertools.chain('<root>', doc.readlines(), '</root>')\n",
    "    root = xml.etree.ElementTree.fromstringlist(it,parser=lxml.etree.XMLParser(recover=True))\n",
    "    for child in root:\n",
    "        if child.attrib.has_key('label'):\n",
    "            yield child.attrib['label'], child.text\n",
    "            continue\n",
    "        yield child.text\n",
    "\n",
    "def process_file():\n",
    "    \"\"\"\n",
    "    读取训练数据和测试数据，并对它们做一些预处理\n",
    "    \"\"\"    \n",
    "    train_pos_file = \"data/train.positive.txt\"\n",
    "    train_neg_file = \"data/train.negative.txt\"\n",
    "    test_comb_file = \"data/test.combined.txt\"\n",
    "    \n",
    "    # TODO: 读取文件部分，把具体的内容写入到变量里面\n",
    "    \n",
    "\n",
    "\n",
    "    train_comments = []\n",
    "    train_labels = []\n",
    "    test_comments = []\n",
    "    test_labels = []\n",
    "\n",
    "    for label, comment in read_file(test_comb_file):\n",
    "        test_labels.append(int(label))\n",
    "        test_comments.append(comment)\n",
    "    \n",
    "    for comment in read_file(train_pos_file):\n",
    "        train_comments.append(comment)\n",
    "        train_labels.append(1)\n",
    "        \n",
    "    for comment in read_file(train_neg_file):\n",
    "        train_comments.append(comment)\n",
    "        train_labels.append(0)\n",
    "    return train_comments, train_labels,test_comments,test_labels\n",
    "\n",
    "def twoSampZ(s1,s2,mudiff):\n",
    "    from numpy import sqrt, abs, round\n",
    "    from scipy.stats import norm\n",
    "    X1 = s1.mean()\n",
    "    X2 = s2.mean()\n",
    "    sd1 = s1.std()\n",
    "    sd2 = s2.std()\n",
    "    n1 = s1.count()\n",
    "    n2 = s2.count()\n",
    "    pooledSE = sqrt(sd1**2/n1 + sd2**2/n2)\n",
    "    z = ((X1 - X2) - mudiff)/pooledSE\n",
    "    pval = 2*(1 - norm.cdf(abs(z)))\n",
    "    return z, pval\n",
    "\n",
    "def pre_process(stopword,sentence_list):\n",
    "    result_list = []\n",
    "    for sentence in sentence_list:\n",
    "        sentence_cut = []\n",
    "        for cut in jieba.cut(sentence):\n",
    "            word = re.sub(r'[^\\u4e00-\\u9fa5]', '', cut)\n",
    "            if len(word) > 0 and word not in stopword:\n",
    "                sentence_cut.append(word)\n",
    "        result_list.append(sentence_cut)\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explorary Analysis: 做一些简单的可视化分析 （10分） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8065 2500\n"
     ]
    }
   ],
   "source": [
    "# 训练数据和测试数据大小\n",
    "train_comments, train_labels,test_comments,test_labels = process_file()\n",
    "print (len(train_comments), len(test_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive samples: mean=3.654828 std=0.939751 n=5000.000000\n",
      "negative samples: mean=3.774089 std=0.872872 n=3065.000000\n",
      "z = -5.783609,p = 0.000000\n",
      "双样本z test显示mean有显著差距，具体应该是样本数量足够大，但是从绝对值上看，相差不大\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: 对于训练数据中的正负样本，分别画出一个histogram， histogram的x抽是每一个样本中字符串的长度，y轴是拥有这个长度的样本的百分比。\n",
    "#       并说出样本长度是否对情感有相关性 (需要先用到结巴分词)\n",
    "#       参考：https://en.wikipedia.org/wiki/Histogram\n",
    "positive_length = []\n",
    "negative_length = []\n",
    "for i in range(len(train_comments)):\n",
    "    if train_labels[i] == 1:\n",
    "        positive_length.append(len(train_comments[i]))\n",
    "    else:\n",
    "        negative_length.append(len(train_comments[i]))\n",
    "log_positive_length = pandas.Series([math.log(l) for l in positive_length])\n",
    "log_negative_length = pandas.Series([math.log(l) for l in negative_length])\n",
    "matplotlib.pyplot.subplot(211)\n",
    "matplotlib.pyplot.hist(log_positive_length)\n",
    "matplotlib.pyplot.subplot(212)\n",
    "matplotlib.pyplot.hist(log_negative_length)\n",
    "z,p = twoSampZ(log_positive_length,log_negative_length , 0)\n",
    "print('positive samples: mean=%f std=%f n=%f'%(log_positive_length.mean(),log_positive_length.std(),log_positive_length.count()))\n",
    "print('negative samples: mean=%f std=%f n=%f'%(log_negative_length.mean(),log_negative_length.std(),log_negative_length.count()))\n",
    "print('z = %.6f,p = %.6f'%(z,p))\n",
    "print('双样本z test显示mean有显著差距，具体应该是样本数量足够大，但是从绝对值上看，相差不大')\n",
    "matplotlib.pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache D:\\Mengbo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.903 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive:  [('很', 2365), ('也', 1354), ('在', 1340), ('有', 1287), ('都', 1177), ('好', 1108), ('不错', 1076), ('就', 981), ('买', 950), ('喜欢', 914), ('这', 866), ('和', 815), ('看', 719), ('还', 719), ('不', 707), ('人', 607), ('没有', 578), ('非常', 556), ('可以', 550), ('还是', 538)]\n",
      "negative:  [('不', 997), ('买', 871), ('就', 855), ('也', 850), ('都', 827), ('很', 798), ('有', 762), ('在', 759), ('没有', 693), ('还', 540), ('卓越', 535), ('说', 487), ('这', 461), ('和', 427), ('好', 417), ('看', 408), ('啊', 355), ('人', 353), ('这个', 344), ('上', 331)]\n"
     ]
    }
   ],
   "source": [
    "# TODO： 分别列出训练数据中正负样本里的top 20单词（可以做适当的stop words removal）。 \n",
    "stopword_list='的了是我'\n",
    "stopword = {x for x in stopword_list}\n",
    "\n",
    "pos_word = {}\n",
    "neg_word = {}\n",
    "train_comments_cut = []\n",
    "test_comments_cut = []\n",
    "train_comments_cut = pre_process(stopword,train_comments)\n",
    "test_comments_cut = pre_process(stopword,test_comments)\n",
    "\n",
    "for i in range(len(train_comments)):\n",
    "    if train_labels[i] == 1:\n",
    "        for word in train_comments_cut[i]:\n",
    "            if word in pos_word.keys():\n",
    "                pos_word[word] += 1\n",
    "            else:\n",
    "                pos_word[word] = 1\n",
    "    else:\n",
    "        for word in train_comments_cut[i]:\n",
    "            if word in neg_word.keys():\n",
    "                neg_word[word] += 1\n",
    "            else:\n",
    "                neg_word[word] = 1\n",
    "\n",
    "pos_word = sorted(pos_word.items(), key=operator.itemgetter(1), reverse=True)\n",
    "neg_word = sorted(neg_word.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print('positive: ',pos_word[:20])\n",
    "print('negative: ',neg_word[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Text Cleaning: 文本处理部分 （10分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO：对于train_comments, test_comments进行字符串的处理，几个考虑的点：\n",
    "#   1. 停用词过滤\n",
    "#   2. 去掉特殊符号\n",
    "#   3. 去掉数字（比如价格..)\n",
    "#   4. ...\n",
    "#   需要注意的点是，由于评论数据本身很短，如果去掉的太多，很可能字符串长度变成0\n",
    "#   预处理部分，可以自行选择合适的方.\n",
    "train_comments_new = []\n",
    "test_comments_new = []\n",
    "y_train = [] # 训练数据的label\n",
    "y_test = [] # 测试数据的label\n",
    "for i in range(len(train_comments_cut)):\n",
    "    if len(train_comments_cut[i])>3:\n",
    "        train_comments_new.append(' '.join(train_comments_cut[i]))\n",
    "        y_train.append(train_labels[i])\n",
    "for i in range(len(test_comments_cut)):\n",
    "    if len(test_comments_cut[i])>3:\n",
    "        test_comments_new.append(' '.join(test_comments_cut[i]))\n",
    "        y_test.append(test_labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction : 从文本中提取特征 （10分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7438, 23257) (2319, 23257) (7438,) (2319,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: 利用tf-idf从文本中提取特征,写到数组里面. \n",
    "#       参考：https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()  # 定义一个tf-idf的vectorizer\n",
    "X_train = vectorizer.fit_transform(train_comments_new) # 训练数据的特征\n",
    "X_test = vectorizer.transform(test_comments_new) # 测试数据的特征\n",
    "\n",
    "\n",
    "print (numpy.shape(X_train), numpy.shape(X_test), numpy.shape(y_train), numpy.shape(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling: 训练模型以及选择合适的超参数 （25分）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'C': 1.0}\n",
      "make_scorer(f1_score)  : 0.8263340281241724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.55      0.67      1176\n",
      "           1       0.66      0.91      0.77      1143\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      2319\n",
      "   macro avg       0.76      0.73      0.72      2319\n",
      "weighted avg       0.76      0.73      0.72      2319\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# TODO： 利用逻辑回归来训练模型\n",
    "#       1. 评估方式： F1-score\n",
    "#       2. 超参数（hyperparater）的选择利用grid search https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "#       3. 打印出在测试数据中的最好的结果（precision, recall, f1-score, 需要分别打印出正负样本，以及综合的）\n",
    "#       请注意：做交叉验证时绝对不能用测试数据。 测试数据只能用来最后的”一次性“检验。\n",
    "#       逻辑回归的使用方法请参考：http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "#       对于逻辑回归，经常调整的超参数为： C\n",
    "\n",
    "grid={\"C\":numpy.logspace(-3,3,7)}\n",
    "logreg=sklearn.linear_model.LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "logreg_cv=sklearn.model_selection.GridSearchCV(logreg,grid,cv=10,scoring='f1')\n",
    "logreg_cv.fit(X_train,y_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(logreg_cv.scorer_,\" :\",logreg_cv.best_score_)\n",
    "y_pred = logreg_cv.predict(X_test)\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'C': 1000, 'gamma': 0.001, 'kernel': 'sigmoid'}\n",
      "make_scorer(f1_score)  : 0.8251457700623123\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.61      0.70      1176\n",
      "           1       0.68      0.88      0.77      1143\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      2319\n",
      "   macro avg       0.76      0.74      0.74      2319\n",
      "weighted avg       0.76      0.74      0.74      2319\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "# TODO： 利用SVM来训练模型\n",
    "#       1. 评估方式： F1-score\n",
    "#       2. 超参数（hyperparater）的选择利用grid search https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "#       3. 打印出在测试数据中的最好的结果（precision, recall, f1-score, 需要分别打印出正负样本，以及综合的）\n",
    "#       请注意：做交叉验证时绝对不能用测试数据。 测试数据只能用来最后的”一次性“检验。\n",
    "#       SVM的使用方法请参考：http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "#       对于SVM模型，经常调整的超参数为：C, gamma, kernel\n",
    "\n",
    "        \n",
    "grid=[{'kernel': ['rbf','poly','sigmoid'], 'gamma': [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "                 'C': [1, 10, 100, 1000]}]\n",
    "svm=sklearn.svm.SVC()\n",
    "svm_cv=sklearn.model_selection.GridSearchCV(svm,grid,cv=10,scoring='f1')\n",
    "svm_cv.fit(X_train,y_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",svm_cv.best_params_)\n",
    "print(svm_cv.scorer_,\" :\",svm_cv.best_score_)\n",
    "y_pred = svm_cv.predict(X_test)\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于超参数的调整，我们经常使用gridsearch，这也是工业界最常用的方法，但它的缺点是需要大量的计算，所以近年来这方面的研究也成为了重点。 其中一个比较经典的成果为Bayesian Optimization（利用贝叶斯的思路去寻找最好的超参数）。Ryan P. Adams主导的Bayesian Optimization利用高斯过程作为后验概率（posteior distribution）来寻找最优解。 https://papers.nips.cc/paper/4522-practical-bayesian-optimization-of-machine-learning-algorithms.pdf 在下面的练习中，我们尝试使用Bayesian Optimization工具来去寻找最优的超参数。参考工具：https://github.com/fmfn/BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   |     C     |   gamma   |\n",
      "-------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.6999  \u001b[0m | \u001b[0m 3.037   \u001b[0m | \u001b[0m 2.179   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.7719  \u001b[0m | \u001b[95m 2.425   \u001b[0m | \u001b[95m-0.9167  \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.7613  \u001b[0m | \u001b[0m-3.431   \u001b[0m | \u001b[0m-1.745   \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.7613  \u001b[0m | \u001b[0m-3.586   \u001b[0m | \u001b[0m-2.827   \u001b[0m |\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.7252  \u001b[0m | \u001b[0m 3.894   \u001b[0m | \u001b[0m-0.7129  \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.7613  \u001b[0m | \u001b[0m-4.0     \u001b[0m | \u001b[0m 3.0     \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.7613  \u001b[0m | \u001b[0m 0.5803  \u001b[0m | \u001b[0m-3.0     \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.7229  \u001b[0m | \u001b[0m-0.8745  \u001b[0m | \u001b[0m 1.741   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.7613  \u001b[0m | \u001b[0m-4.0     \u001b[0m | \u001b[0m 0.6979  \u001b[0m |\n",
      "| \u001b[95m 10      \u001b[0m | \u001b[95m 0.8112  \u001b[0m | \u001b[95m 0.3567  \u001b[0m | \u001b[95m-1.102   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.7613  \u001b[0m | \u001b[0m-0.875   \u001b[0m | \u001b[0m-1.623   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.7863  \u001b[0m | \u001b[0m 4.0     \u001b[0m | \u001b[0m-3.0     \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.7793  \u001b[0m | \u001b[0m 0.832   \u001b[0m | \u001b[0m 0.12    \u001b[0m |\n",
      "| \u001b[95m 14      \u001b[0m | \u001b[95m 0.8235  \u001b[0m | \u001b[95m 2.514   \u001b[0m | \u001b[95m-3.0     \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.7032  \u001b[0m | \u001b[0m 4.0     \u001b[0m | \u001b[0m 3.0     \u001b[0m |\n",
      "=================================================\n",
      "Final result: {'target': 0.8234636532580888, 'params': {'C': 2.5138460483214984, 'gamma': -3.0}}\n"
     ]
    }
   ],
   "source": [
    "# TODO: 仍然使用SVM模型，但在这里使用Bayesian Optimization来寻找最好的超参数。 \n",
    "#       1. 评估方式： F1-score\n",
    "#       2. 超参数（hyperparater）的选择利用Bayesian Optimization https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "#       3. 打印出在测试数据中的最好的结果（precision, recall, f1-score, 需要分别打印出正负样本，以及综合的）\n",
    "#       请注意：做交叉验证时绝对不能用测试数据。 测试数据只能用来最后的”一次性“检验。\n",
    "#       SVM的使用方法请参考：http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "#       对于SVM模型，经常调整的超参数为：C, gamma, kernel\n",
    "#       参考Bayesian Optimization开源工具： https://github.com/fmfn/BayesianOptimization\n",
    "\n",
    "def optimizer_svc(X,y):\n",
    "    def svc_cv(C,gamma):\n",
    "        svm=sklearn.svm.SVC(C=10 ** C,gamma=10 ** gamma,kernel='sigmoid')\n",
    "        return sklearn.model_selection.cross_val_score(svm,X=X,y=y,scoring='f1',cv=10).mean()\n",
    "    grid={'gamma': (-3, 3),'C': (-4, 4)}\n",
    "    optimizer = bayes_opt.BayesianOptimization(f=svc_cv,pbounds = grid,verbose=2)\n",
    "    optimizer.maximize(n_iter=10)\n",
    "    return optimizer\n",
    "result = optimizer_svc(X_train,y_train)\n",
    "print(\"Final result:\",result.max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 特征: 添加n-gram特征 (10分)\n",
    "在原有tf-idf特征的基础上，添加n-gram特征（在这里我们使用bi-gram特征）。添加完之后效果是否有提升？ 为什么？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7973, 163599) (2479, 163599) (7973,) (2479,)\n"
     ]
    }
   ],
   "source": [
    "train_comments, train_labels, test_comments, test_labels = process_file()\n",
    "train_comments_cut = pre_process(set(), train_comments)\n",
    "test_comments_cut = pre_process(set(), test_comments)\n",
    "\n",
    "train_comments_new = []\n",
    "y_train = []\n",
    "for i in range(len(train_comments_cut)):\n",
    "    cut = train_comments_cut[i]\n",
    "    if len(cut)>1:\n",
    "        bi_gram = []\n",
    "        for j in range(len(cut)-1):\n",
    "            bi_gram.append(cut[j]+cut[j+1])\n",
    "        train_comments_new.append(' '.join(cut+bi_gram))\n",
    "        y_train.append(train_labels[i])\n",
    "\n",
    "test_comments_new = []\n",
    "y_test = []\n",
    "for i in range(len(test_comments_cut)):\n",
    "    cut = test_comments_cut[i]\n",
    "    if len(cut)>1:\n",
    "        bi_gram = []\n",
    "        for j in range(len(cut)-1):\n",
    "            bi_gram.append(cut[j]+cut[j+1])\n",
    "        test_comments_new.append(' '.join(cut+bi_gram))\n",
    "        y_test.append(test_labels[i])\n",
    "\n",
    "\n",
    "vectorizer = sklearn.feature_extraction.text.TfidfVectorizer()  # 定义一个tf-idf的vectorizer\n",
    "X_train = vectorizer.fit_transform(train_comments_new)  # 结果存放在X矩阵\n",
    "X_test = vectorizer.transform(test_comments_new)\n",
    "\n",
    "print (numpy.shape(X_train), numpy.shape(X_test), numpy.shape(y_train), numpy.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hpyerparameters :(best parameters)  {'C': 10.0}\n",
      "make_scorer(f1_score)  : 0.8398903690360119\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.63      0.73      1239\n",
      "           1       0.71      0.89      0.79      1240\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      2479\n",
      "   macro avg       0.78      0.76      0.76      2479\n",
      "weighted avg       0.78      0.76      0.76      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TODO 模型的训练，如上\n",
    "grid={\"C\":numpy.logspace(-3,3,7)}\n",
    "logreg=sklearn.linear_model.LogisticRegression(solver='lbfgs',max_iter=1000)\n",
    "logreg_cv=sklearn.model_selection.GridSearchCV(logreg,grid,cv=10,scoring='f1')\n",
    "logreg_cv.fit(X_train,y_train)\n",
    "print(\"tuned hpyerparameters :(best parameters) \",logreg_cv.best_params_)\n",
    "print(logreg_cv.scorer_,\" :\",logreg_cv.best_score_)\n",
    "y_pred = logreg_cv.predict(X_test)\n",
    "print(sklearn.metrics.classification_report(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
